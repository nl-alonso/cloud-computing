# Core ML libraries for LLaMA 3.1 and LLaMA 4 Scout
torch>=2.1.0
transformers>=4.40.0
peft>=0.10.0
datasets>=2.18.0
evaluate>=0.4.0
accelerate>=0.29.0

# Quantization support
bitsandbytes>=0.43.0

# LLaMA 4 Scout support
llama-stack>=0.0.1  # Meta's official LLaMA Stack CLI

# Google Cloud dependencies
google-cloud-aiplatform>=1.42.0
google-cloud-storage>=2.10.0

# Monitoring and logging
wandb>=0.16.0
tensorboard>=2.14.0

# Data processing
numpy>=1.24.0
pandas>=2.1.0
scikit-learn>=1.3.0

# Utilities
tqdm>=4.65.0
requests>=2.31.0
packaging>=23.0

# Performance optimization for LLaMA 3.1 and 4 Scout
flash-attn>=2.5.0  # Flash Attention 2 for faster training
ninja>=1.11.0  # For compiling Flash Attention
triton>=2.1.0  # Required for Flash Attention operations

# Additional dependencies for LLaMA models
sentencepiece>=0.1.99  # For tokenization
protobuf>=3.20.0  # Protocol buffers
safetensors>=0.4.0  # Safe tensor serialization

# Development tools
jupyter>=1.0.0
matplotlib>=3.7.0
seaborn>=0.12.0

# Optional: For mixed precision training
apex  # NVIDIA Apex for advanced mixed precision (optional) 